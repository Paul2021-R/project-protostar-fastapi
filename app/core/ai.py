import os
import glob
from textwrap import dedent
from openai import AsyncOpenAI
from core.config import settings

# [ì „ì—­ ë³€ìˆ˜] ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ìª¼ê°œì§„ ì§€ì‹ ì¡°ê°ë“¤ (Chunks)
KNOWLEDGE_CHUNKS = []

client = AsyncOpenAI(
    api_key=settings.OPENROUTER_API_KEY,
    base_url="https://openrouter.ai/api/v1",
    default_headers={
        "HTTP-Referer": settings.SITE_URL, 
        "X-Title": settings.SITE_NAME,
    }
)

def load_and_chunk_files(directory: str):
    """
    MD íŒŒì¼ì„ ì½ì–´ì„œ 'ë¬¸ë‹¨(\n\n)' ë‹¨ìœ„ë¡œ ìª¼ê°œì„œ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥í•¨.
    ì´ê²Œ RAGì˜ í•µì‹¬ì¸ 'Chunking' ê³¼ì •ì…ë‹ˆë‹¤.
    """
    chunks = []
    file_paths = glob.glob(os.path.join(directory, "*.md"))
    
    for file_path in file_paths:
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()
                filename = os.path.basename(file_path)
                
                # 1. ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (ë¹ˆ ì¤„ ê¸°ì¤€)
                raw_chunks = content.split("\n\n")
                
                # 2. ì˜ë¯¸ ìˆëŠ” ë‚´ìš©ë§Œ ì €ì¥
                for i, text in enumerate(raw_chunks):
                    if len(text.strip()) > 10:  # ë„ˆë¬´ ì§§ì€ ê±´ ë¬´ì‹œ
                        chunks.append(f"[Source: {filename} / Para {i+1}]\n{text.strip()}")
        except Exception as e:
            print(f"âš ï¸ Error loading {file_path}: {e}")
            
    return chunks

async def init_ai_context():
    global KNOWLEDGE_CHUNKS
    base_dir = "prompts"
    
    print(f"ğŸ“‚ Chunking Knowledge Base from {base_dir}/user_data/...")
    KNOWLEDGE_CHUNKS = load_and_chunk_files(os.path.join(base_dir, "user_data"))
    
    print(f"âœ… Total Knowledge Chunks: {len(KNOWLEDGE_CHUNKS)}")


def retrieve_relevant_chunks(query: str, top_k: int = 3) -> str:
    """
    [Retrieval] ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ë‹¨ë§Œ ì°¾ì•„ë‚´ëŠ” ê²€ìƒ‰ ì—”ì§„
    """
    if not KNOWLEDGE_CHUNKS:
        return ""

    query_tokens = set(query.split()) # ì§ˆë¬¸ì„ ë‹¨ì–´ë¡œ ìª¼ê°¬
    scores = []

    for chunk in KNOWLEDGE_CHUNKS:
        # ë¬¸ë‹¨ ì•ˆì— ì§ˆë¬¸ì˜ ë‹¨ì–´ê°€ ëª‡ ê°œë‚˜ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ì ìˆ˜ ê³„ì‚°
        score = sum(1 for token in query_tokens if token in chunk)
        if score > 0:
            scores.append((score, chunk))
    
    # ì ìˆ˜ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬í•´ì„œ top_kê°œë§Œ ë½‘ìŒ
    scores.sort(key=lambda x: x[0], reverse=True)
    top_results = [item[1] for item in scores[:top_k]]
    
    if not top_results:
        return "" # ê´€ë ¨ ë‚´ìš©ì´ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜

    return "\n\n---\n\n".join(top_results)


async def generate_response(prompt: str, context: str = ''):
    # 1. Retrieval (ê²€ìƒ‰): ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ìë£Œë§Œ ê°€ì ¸ì˜¤ê¸°
    # ì‚¬ìš©ìê°€ ì§ì ‘ ë„˜ê²¨ì¤€ contextê°€ ìˆìœ¼ë©´ ê·¸ê±¸ ìš°ì„ , ì—†ìœ¼ë©´ DBì—ì„œ ê²€ìƒ‰
    # found_context = context if context else retrieve_relevant_chunks(prompt)

    # # 2. Generation (ìƒì„±): ì°¾ì€ ìë£Œê°€ ì—†ìœ¼ë©´ ë°”ë¡œ ëª¨ë¥¸ë‹¤ê³  í•˜ê¸°
    # if not found_context:
    #     return "ì£„ì†¡í•©ë‹ˆë‹¤. í•™ìŠµëœ ë¬¸ì„œ ë‚´ì—ì„œ í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # 3. í”„ë¡¬í”„íŠ¸ ì¡°ë¦½ (ìë£Œê°€ ìˆìœ¼ë‹ˆ ë‹µë³€ ìƒì„±)
    # full_prompt = dedent(f"""
    # <relevant_documents>
    # {found_context}
    # </relevant_documents>

    # <instruction>
    # You are 'Protostar', a strict AI assistant.
    # Answer the user's question using **ONLY** the information in <relevant_documents>.
    
    # Rules:
    # 1. If the exact answer is not in the documents, say "ë¬¸ì„œì— ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."
    # 2. Do NOT summarize the whole document, just answer the specific question.
    # 3. Answer in Korean.
    # </instruction>

    # <user_question>
    # {prompt}
    # </user_question>
    # """).strip()
    full_prompt = dedent(f"""
    <relevant_documents>

    </relevant_documents>

    <instruction>
    You are 'Protostar', a strict and helpful AI assistant.
    
    Rules:
    1. Answer in Korean.
    2. ë‹¹ì‹ ì€ í˜„ì¬ ë¸”ë¡œê·¸ ìƒì˜ ì±—ë´‡ ì„œë¹„ìŠ¤ì´ë©° Protostar ë¼ëŠ” ì´ë¦„ì„ ê°–ê³  ìˆëŠ” ì§€ì› AI ì…ë‹ˆë‹¤. 
    3. ë‹¹ì‹ ì˜ ì—­í• ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
        - ë‹¹ì‹ ì—ê²Œ ì‚¬ì „ ìë£Œê°€ ì¡´ì¬í•œë‹¤ë©´ í•´ë‹¹ ìë£Œë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ì—¬ ì´ìš©ìë“¤ì˜ ì´ë ¥ì„œ, ê²½ë ¥, ëŠ¥ë ¥ì¹˜ë¥¼ ì§ˆë¬¸ìì—ê²Œ ì–´í•„í•˜ê±°ë‚˜ ì†Œê°œí•©ë‹ˆë‹¤. 
        - ë‹¹ì‹ ì—ê²Œ ë¸”ë¡œê·¸ ìƒì—ì„œ ì œê³µë˜ëŠ” ìë£Œì— ëŒ€í•´ ë‹µë³€ì„ ìš”ì²­í•  ê²½ìš° ì´ì— ë§ì¶° ë‹µë³€ì„ í•´ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤. í•µì‹¬ íŒŒì•…, ìš”ì•½ ë“±ì˜ ë‹µë³€ì„ í•´ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤. 
        - ë¸”ë¡œê·¸ë‚˜ ê°œì¸ì˜ ì´ë ¥ê³¼ ê´€ë ¨ë˜ì§€ ì•Šì€ ì¼ë°˜ì ì¸ ì§ˆë¬¸ì—ëŠ” 'ê¶Œí•œ ì—†ìŒ' ì´ë€ ì´ìœ  í•˜ì— ë‹µë³€ì„ í•˜ì§€ ë§ì•„ì•¼ í•©ë‹ˆë‹¤. 
    4. ì±—ë´‡ì˜ í™˜ê²½ì—ì„œ ì œê³µë˜ë¯€ë¡œ í…ìŠ¤íŠ¸ ë‹µë³€ë§Œ í•´ì£¼ì–´ì•¼ í•˜ë©° ê°•ì¡° í‘œí˜„ì„ ë¹„ë¡¯í•œ ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ ë³€í™”ëŠ” í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. 
    5. ëª¨ë“  ë‹µë³€ì—ì„œ í•µì‹¬ì€, ì§ˆë¬¸ìì˜ ìš”ì§€ì— ëŒ€í•œ ê²°ë¡ ì„ ìš°ì„  ì œì‹œí•˜ë©° ê·¼ê±°ë‚˜ ë‚´ìš©ì€ í•˜ìœ„ì— ê¸°ì¬í•©ë‹ˆë‹¤. 
    6. ëª¨ë“  ë‹µë³€ì˜ í˜•íƒœëŠ” ê³µì†í•˜ê³ , ì¹œì ˆí•˜ë©°, ì´ëª¨í‹°ì½˜ì„ í™œìš©í•´ì•¼ í•˜ë©°, ê°€ëŠ¥í•œ ì–‘ì€ 3ë¬¸ë‹¨ ì´í•˜ë¡œ ì‘ì„±ì´ í•„ìš”í•©ë‹ˆë‹¤. 
    </instruction>

    <user_question>
    {prompt}
    </user_question>
    """).strip()


    try:
        response = await client.chat.completions.create(
            model=settings.OPENROUTER_MODEL,
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ Protostar AI ì—ì´ì „íŠ¸ ë¹„ì„œë¡œì„œ ì„œë¹„ìŠ¤ë¥¼ ë¸”ë¡œê·¸ì— íƒ‘ì¬ë˜ì–´ ìˆì–´ì„œ, ì´ìš©ìì˜ ì´ë ¥ ì–´í•„ ë¸”ë¡œê·¸ ê¸€ì„ ì²¨ë¶€ ì‹œ ì§ˆë¬¸ìì˜ ìš”ì²­ì— ë§ì¶° ë‹µë³€í•˜ê¸°ë¥¼ í•´ì£¼ëŠ” ë¹„ì„œì…ë‹ˆë‹¤."},
                {"role": "user", "content": full_prompt}
            ],
            temperature=0.7, # ì‚¬ì‹¤ ê¸°ë°˜ ë‹µë³€
        )

        print(response.choices)

        return response.choices[0].message.content
        
    except Exception as e:
        return f"âŒ AI Error: {str(e)}"