import os
import glob
import logging
from textwrap import dedent
from openai import AsyncOpenAI
from core.config import settings

# [ì „ì—­ ë³€ìˆ˜] ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ìª¼ê°œì§„ ì§€ì‹ ì¡°ê°ë“¤ (Chunks)
KNOWLEDGE_CHUNKS = []

logger = logging.getLogger("uvicorn")

client = AsyncOpenAI(
    api_key=settings.OPENROUTER_API_KEY,
    base_url="https://openrouter.ai/api/v1",
    default_headers={
        "HTTP-Referer": settings.SITE_URL, 
        "X-Title": settings.SITE_NAME,
    }
)

def load_and_chunk_files(directory: str):
    """
    MD íŒŒì¼ì„ ì½ì–´ì„œ 'ë¬¸ë‹¨(\n\n)' ë‹¨ìœ„ë¡œ ìª¼ê°œì„œ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥í•¨.
    ì´ê²Œ RAGì˜ í•µì‹¬ì¸ 'Chunking' ê³¼ì •ì…ë‹ˆë‹¤.
    """
    chunks = []
    file_paths = glob.glob(os.path.join(directory, "*.md"))
    
    for file_path in file_paths:
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()
                filename = os.path.basename(file_path)
                
                # 1. ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (ë¹ˆ ì¤„ ê¸°ì¤€)
                raw_chunks = content.split("\n\n")
                
                # 2. ì˜ë¯¸ ìˆëŠ” ë‚´ìš©ë§Œ ì €ì¥
                for i, text in enumerate(raw_chunks):
                    if len(text.strip()) > 10:  # ë„ˆë¬´ ì§§ì€ ê±´ ë¬´ì‹œ
                        chunks.append(f"[Source: {filename} / Para {i+1}]\n{text.strip()}")
        except Exception as e:
            logger.error(f"âš ï¸ Error loading {file_path}: {e}")
            
    return chunks

async def generate_response_stream(
    prompt: str, 
    mode: str = 'general',
    context: str = '', # worker.pyì—ì„œ ê²€ìƒ‰ëœ RAG ë°ì´í„°ê°€ ì—¬ê¸°ì— ë“¤ì–´ì˜µë‹ˆë‹¤.
    history: list[dict] = None
):
    if history is None:
        history = []

    # ---------------------------------------------------------
    # 1. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„± (í˜ë¥´ì†Œë‚˜ & ê·œì¹™ ì •ì˜)
    # ---------------------------------------------------------
    # context(RAG ê²€ìƒ‰ ê²°ê³¼)ê°€ ìˆìœ¼ë©´ í¬í•¨ì‹œí‚¤ê³ , ì—†ìœ¼ë©´ 'ì •ë³´ ì—†ìŒ' ì²˜ë¦¬
    context_block = ""
    if context:
        context_block = f"""
<relevant_documents>
{context}
</relevant_documents>
"""

    system_instruction = dedent(f"""
    ë‹¹ì‹ ì€ ë¥˜í•œì†” ê°œë°œìì˜ ê¸°ìˆ  ë¸”ë¡œê·¸ ë° í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ ë‹´ë‹¹í•˜ëŠ” AI ë¹„ì„œ 'Protostar(í”„ë¡œí† ìŠ¤íƒ€)'ì…ë‹ˆë‹¤.
    
    [Role & Purpose]
    - ë‹¹ì‹ ì˜ ì£¼ ëª©ì ì€ ì§ˆë¬¸ìì—ê²Œ **ë¥˜í•œì†”(Paul)** ì˜ ê²½ë ¥, ê¸°ìˆ  ìŠ¤íƒ, í”„ë¡œì íŠ¸ ê²½í—˜ì„ ì „ë‹¬í•´ì£¼ëŠ” ê²ƒì…ë‹ˆë‹¤.
    - ì œê³µëœ <relevant_documents> ì •ë³´ë¥¼ ìµœìš°ì„  ê·¼ê±°ë¡œ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.
    
    [Strict Rules]
    1. **Language**: **í•œêµ­ì–´**ë¡œ ë‹µë³€í•˜ì‹­ì‹œì˜¤. ë‹¤ë¥¸ ì–¸ì–´ê°€ ë“¤ì–´ì˜¬ ë•Œë§Œ ì´ì— ë§ê²Œ ëŒ€ì‘í•˜ì‹­ì‹œì˜¤.
    2. **Context First**: 
       - <relevant_documents>ì— ìˆëŠ” ë‚´ìš©ì´ë¼ë©´, í•´ë‹¹ ë‚´ìš©ì„ ìš”ì•½ ë° ì¸ìš©í•˜ì—¬ ì „ë¬¸ì ìœ¼ë¡œ ë‹µë³€í•˜ì‹­ì‹œì˜¤.
       - ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì´ì§€ë§Œ ê°œë°œ/IT ì¼ë°˜ ìƒì‹ì´ë¼ë©´ ë‹µë³€í•˜ë˜, "ì œê³µëœ ë¬¸ì„œì—ëŠ” ì—†ì§€ë§Œ ì¼ë°˜ì ì¸ ì§€ì‹ìœ¼ë¡œëŠ”..." ì´ë¼ê³  ì„œë‘ë¥¼ ë–¼ì‹­ì‹œì˜¤.
       - **ë¸”ë¡œê·¸/ì´ë ¥/ê°œë°œê³¼ ì „í˜€ ë¬´ê´€í•œ ì§ˆë¬¸**(ì˜ˆ: ì˜¤ëŠ˜ ì ì‹¬ ë©”ë‰´ ì¶”ì²œ, ì—°ì˜ˆì¸ ê°€ì‹­ ë“±)ì—ëŠ” "ì£„ì†¡í•©ë‹ˆë‹¤. ì €ëŠ” ê¸°ìˆ  ë¸”ë¡œê·¸ ì•ˆë‚´ë¥¼ ìœ„í•œ AIì´ë¯€ë¡œ í•´ë‹¹ ì§ˆë¬¸ì—ëŠ” ë‹µë³€ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤."ë¼ê³  ì •ì¤‘íˆ ê±°ì ˆí•˜ì‹­ì‹œì˜¤.
    3. **Tone & Manner**:
       - ê³µì†í•˜ê³  ì¹œì ˆí•˜ë©° ì „ë¬¸ì ì¸ 'ë¹„ì„œ'ì˜ ë§íˆ¬ë¥¼ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤.
       - ì ì ˆí•œ ì´ëª¨ì§€(ğŸ˜Š, ğŸ’¡, ğŸš€ ë“±)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë”±ë”±í•˜ì§€ ì•Šê²Œ ë‹µë³€í•˜ì‹­ì‹œì˜¤.
    4. **Format**:
       - í•µì‹¬ ê²°ë¡ ì„ ë¨¼ì € ì œì‹œí•˜ê³ (ë‘ê´„ì‹), ë¶€ì—° ì„¤ëª…ì„ í•˜ìœ„ì— ì‘ì„±í•˜ì‹­ì‹œì˜¤.
       - ë‹µë³€ì€ ê°€ë…ì„±ì„ ìœ„í•´ 3ë¬¸ë‹¨ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ êµ¬ì„±í•˜ì‹­ì‹œì˜¤.
       - ë‹µë³€ ì–‘ì‹ìœ¼ë¡œ Markdown ë¬¸ë²•ì€ ì“°ì§€ ë§ë©°, ë„ì›Œì“°ê¸°, ì¤„ë°”ê¿ˆë“±ì„ í¬í•¨í•œ ì¼ë°˜ í…ìŠ¤íŠ¸ ë°©ì‹ìœ¼ë¡œ ë‹µë³€í•˜ë©°, ê°•ì¡°ê°€ í•„ìš”ì‹œ ', "  ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜, ì œëª©ì„ ì‘ì„± ì‹œ [] ë¥¼ ì‚¬ìš©í•˜ì‹­ì‹œì•„.
    {context_block}
    """).strip()

    # ---------------------------------------------------------
    # 2. ë©”ì‹œì§€ ë°°ì—´ êµ¬ì„±
    # ---------------------------------------------------------
    # [System Message] -> [History] -> [User Question] ìˆœì„œ
    messages = [
        {"role": "system", "content": system_instruction}
    ]
    
    # íˆìŠ¤í† ë¦¬ ì¶”ê°€ (System ë©”ì‹œì§€ ë°”ë¡œ ë’¤ì— ë¶™ì„)
    if history:
        messages.extend(history)
        
    # í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸ ì¶”ê°€
    messages.append({"role": "user", "content": prompt})

    try:
        # ---------------------------------------------------------
        # 3. LLM í˜¸ì¶œ ë° ìŠ¤íŠ¸ë¦¬ë°
        # ---------------------------------------------------------
        stream = await client.chat.completions.create(
            model=settings.OPENROUTER_MODEL, # worker.pyì˜ ì„¤ì •ì„ ë”°ë¦„
            messages=messages,
            stream=True,
            temperature=0.7, # ì°½ì˜ì„±ê³¼ ì‚¬ì‹¤ì„±ì˜ ë°¸ëŸ°ìŠ¤
            # max_tokens=1000, # í•„ìš” ì‹œ ì œí•œ
        )

        async for chunk in stream:
            content = chunk.choices[0].delta.content
            if content:
                yield content

    except Exception as e:
        logger.error(f"âŒ AI Generation Error: {e}")
        raise e

async def generate_summary(original_text: str, model: str = None) -> dict:
    """
    Main Worker ì˜ ë‹µë³€ì„ ìš”ì•½í•˜ëŠ” í•¨ìˆ˜
    - ì…ë ¥ : ì›ë³¸ ë‹µë³€ í…ìŠ¤íŠ¸
    - ì¶œë ¥ : {"summary": "ìš”ì•½ëœ í…ìŠ¤íŠ¸", "usage": {input, output, model}}
    """

    if not original_text:
        return {"summary": "", "usage": {}}

    if len(original_text) < 150:
        return {
            "summary": original_text, 
            "usage": {
                "input": 0,
                "output": 0,
                "model": "bypass"
            }
        }

    system_prompt = dedent("""
    ë‹¹ì‹ ì€ ëŒ€í™” ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. AI ì–´ì‹œìŠ¤í„´íŠ¸ì˜ ë‹µë³€ì„ 3ë¬¸ì¥ ì´ë‚´ì˜ í•œ ë¬¸ë‹¨ìœ¼ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.

    ## ìš”ì•½ ì›ì¹™
    1. **í•µì‹¬ ê²°ë¡ /ë‹µë³€**ì„ ì²« ë¬¸ì¥ì— ë°°ì¹˜
    2. **êµ¬ì²´ì  ë°ì´í„°**(ìˆ«ì, ì´ë¦„, ì½”ë“œëª… ë“±)ëŠ” ë°˜ë“œì‹œ ë³´ì¡´
    3. **ì‚¬ìš©ìê°€ ë‹¤ìŒ ì§ˆë¬¸ì— í™œìš©í•  ë§¥ë½**ì„ ìš°ì„  í¬í•¨

    ## ì œì™¸ ëŒ€ìƒ
    - ì¸ì‚¬ë§, ë¶€ì—° ì„¤ëª…, ì˜ˆì‹œì˜ ìƒì„¸ ë‚´ìš©
    - "~í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤", "~ê²ƒ ê°™ìŠµë‹ˆë‹¤" ë“±ì˜ ì™„ê³¡ í‘œí˜„

    ## ì¶œë ¥ í˜•ì‹
    - í•œ ë¬¸ë‹¨, 3ë¬¸ì¥ ì´ë‚´
    - ì¡´ëŒ“ë§ ì—†ì´ ê°„ê²°í•œ ì •ë³´ ì „ë‹¬ì²´ ì‚¬ìš©
    """).strip()
    
    try:
        target_model = model if model else settings.OPENROUTER_MODEL

        response = await client.chat.completions.create(
            model=target_model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": original_text}
            ],
            stream=False,
            temperature=0.3,
        )

        summary_text = response.choices[0].message.content.strip()
        usage_info = response.usage

        if usage_info:
            input_tokens = usage_info.prompt_tokens
            output_tokens = usage_info.completion_tokens
        else:
            logger.warning("âš ï¸ Usage info missing in API response.")
            input_tokens = 0
            output_tokens = 0

        return {
            "summary": summary_text,
            "usage": {
                "input": input_tokens,
                "output": output_tokens,
                "model": target_model
            }
        }
        
    except Exception as e:
        logger.error(f"âŒ Summary Generation Error: {str(e)}")
        return {
            "summary": original_text[:500],
            "usage": {}
        }