import os
import glob
from textwrap import dedent
from openai import AsyncOpenAI
from core.config import settings

# [ì „ì—­ ë³€ìˆ˜] ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ìª¼ê°œì§„ ì§€ì‹ ì¡°ê°ë“¤ (Chunks)
KNOWLEDGE_CHUNKS = []

client = AsyncOpenAI(
    api_key=settings.OPENROUTER_API_KEY,
    base_url="https://openrouter.ai/api/v1",
    default_headers={
        "HTTP-Referer": settings.SITE_URL, 
        "X-Title": settings.SITE_NAME,
    }
)

def load_and_chunk_files(directory: str):
    """
    MD íŒŒì¼ì„ ì½ì–´ì„œ 'ë¬¸ë‹¨(\n\n)' ë‹¨ìœ„ë¡œ ìª¼ê°œì„œ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥í•¨.
    ì´ê²Œ RAGì˜ í•µì‹¬ì¸ 'Chunking' ê³¼ì •ì…ë‹ˆë‹¤.
    """
    chunks = []
    file_paths = glob.glob(os.path.join(directory, "*.md"))
    
    for file_path in file_paths:
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()
                filename = os.path.basename(file_path)
                
                # 1. ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (ë¹ˆ ì¤„ ê¸°ì¤€)
                raw_chunks = content.split("\n\n")
                
                # 2. ì˜ë¯¸ ìˆëŠ” ë‚´ìš©ë§Œ ì €ì¥
                for i, text in enumerate(raw_chunks):
                    if len(text.strip()) > 10:  # ë„ˆë¬´ ì§§ì€ ê±´ ë¬´ì‹œ
                        chunks.append(f"[Source: {filename} / Para {i+1}]\n{text.strip()}")
        except Exception as e:
            print(f"âš ï¸ Error loading {file_path}: {e}")
            
    return chunks

async def init_ai_context():
    global KNOWLEDGE_CHUNKS
    base_dir = "prompts"
    
    print(f"ğŸ“‚ Chunking Knowledge Base from {base_dir}/user_data/...")
    KNOWLEDGE_CHUNKS = load_and_chunk_files(os.path.join(base_dir, "user_data"))
    
    print(f"âœ… Total Knowledge Chunks: {len(KNOWLEDGE_CHUNKS)}")


def retrieve_relevant_chunks(query: str, top_k: int = 3) -> str:
    """
    [Retrieval] ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ë‹¨ë§Œ ì°¾ì•„ë‚´ëŠ” ê²€ìƒ‰ ì—”ì§„
    """
    if not KNOWLEDGE_CHUNKS:
        return ""

    query_tokens = set(query.split()) # ì§ˆë¬¸ì„ ë‹¨ì–´ë¡œ ìª¼ê°¬
    scores = []

    for chunk in KNOWLEDGE_CHUNKS:
        # ë¬¸ë‹¨ ì•ˆì— ì§ˆë¬¸ì˜ ë‹¨ì–´ê°€ ëª‡ ê°œë‚˜ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ì ìˆ˜ ê³„ì‚°
        score = sum(1 for token in query_tokens if token in chunk)
        if score > 0:
            scores.append((score, chunk))
    
    # ì ìˆ˜ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬í•´ì„œ top_kê°œë§Œ ë½‘ìŒ
    scores.sort(key=lambda x: x[0], reverse=True)
    top_results = [item[1] for item in scores[:top_k]]
    
    if not top_results:
        return "" # ê´€ë ¨ ë‚´ìš©ì´ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜

    return "\n\n---\n\n".join(top_results)


async def generate_response(prompt: str, context: str = ''):
    # 1. Retrieval (ê²€ìƒ‰): ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ìë£Œë§Œ ê°€ì ¸ì˜¤ê¸°
    # ì‚¬ìš©ìê°€ ì§ì ‘ ë„˜ê²¨ì¤€ contextê°€ ìˆìœ¼ë©´ ê·¸ê±¸ ìš°ì„ , ì—†ìœ¼ë©´ DBì—ì„œ ê²€ìƒ‰
    found_context = context if context else retrieve_relevant_chunks(prompt)

    # 2. Generation (ìƒì„±): ì°¾ì€ ìë£Œê°€ ì—†ìœ¼ë©´ ë°”ë¡œ ëª¨ë¥¸ë‹¤ê³  í•˜ê¸°
    if not found_context:
        return "ì£„ì†¡í•©ë‹ˆë‹¤. í•™ìŠµëœ ë¬¸ì„œ ë‚´ì—ì„œ í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # 3. í”„ë¡¬í”„íŠ¸ ì¡°ë¦½ (ìë£Œê°€ ìˆìœ¼ë‹ˆ ë‹µë³€ ìƒì„±)
    full_prompt = dedent(f"""
    <relevant_documents>
    {found_context}
    </relevant_documents>

    <instruction>
    You are 'Protostar', a strict AI assistant.
    Answer the user's question using **ONLY** the information in <relevant_documents>.
    
    Rules:
    1. If the exact answer is not in the documents, say "ë¬¸ì„œì— ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."
    2. Do NOT summarize the whole document, just answer the specific question.
    3. Answer in Korean.
    </instruction>

    <user_question>
    {prompt}
    </user_question>
    """).strip()

    try:
        response = await client.chat.completions.create(
            model=settings.OPENROUTER_MODEL,
            messages=[
                {"role": "user", "content": full_prompt}
            ],
            temperature=1, # ì‚¬ì‹¤ ê¸°ë°˜ ë‹µë³€
        )
        return response.choices[0].message.content
        
    except Exception as e:
        return f"âŒ AI Error: {str(e)}"