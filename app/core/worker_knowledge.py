# app/core/worker_knowledge.py
import asyncio
import json
import logging
import httpx
import re
from openai import AsyncOpenAI
from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter

from core.config import settings
from core.redis import get_redis_client
from core.minio_client import minio_client
from core.database import AsyncSessionLocal
from core.vectorized_doc import VectorizedDoc

logger = logging.getLogger("uvicorn")

MAX_CONCURRENT_JOBS = 3
semaphore = asyncio.Semaphore(MAX_CONCURRENT_JOBS)

ai_client = AsyncOpenAI(
  api_key=settings.OPENROUTER_API_KEY,
  base_url="https://openrouter.ai/api/v1",
)

async def extract_metadata_from_llm(text_preview: str) -> dict:
  """
  ë¬¸ì„œ ì• ë¶€ë¶„ì„ ì½ê³  ë©”íƒ€ ë°ì´í„°ë¥¼ ì¶”ì¶œ 
  """
  system_prompt = """
    Analyze the provided markdown text. 
    Return a JSON object with the following keys:
    - "summary": A one-sentence summary of the content (Korean).
    - "keywords": A list of top 5 key concepts or tech stacks (English/Korean mixed).
    - "category": Choose one from [Technical, Business, General, Memo].
    
    Output JSON only. No markdown formatting.
    """
  try:
    response = await ai_client.chat.completions.create(
      model=settings.OPENROUTER_MODEL,
      messages=[
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"Text:\n{text_preview[:3000]}"} 
      ],
      temperature=0.3,
      response_format={"type": "json_object"}
    )
    
    content = response.choices[0].message.content

    if content.startswith("```"):
      content = re.sub(r"^```json\s*", "", content)
      content = re.sub(r"^```\s*", "", content)
      content = re.sub(r"```$", "", content)
    
    return json.loads(content)
  except Exception as e:
    logger.warning(f"âš ï¸ Metadata extraction failed: {e}")
    # ì‹¤íŒ¨í•´ë„ ì£½ì§€ ì•Šê³  ë¹ˆ ê°’ ë¦¬í„´ (ê¸°ë³¸ ë¡œì§ì€ ëŒì•„ê°€ì•¼ í•˜ë¯€ë¡œ)
    return {
      "summary": "failed",
      "keywords": [],
      "category": "Uncategorized"
      }


async def get_embeddings(text_chunks: list[str]) -> list[list[float]]:
  """
  openRouter ì—ì„œ ë°›ì•„ì„œ ì„ë² ë”© ìƒì„±
  """
  try:
    response = await ai_client.embeddings.create(
      model=settings.OPENROUTER_EMBEDDING_MODEL,
      input=text_chunks,
    )

    return [data.embedding for data in response.data]
  except Exception as e:
    logger.error(f"âŒ OpenRouter Embedding Error: {e}")
    raise e

async def send_webhook(doc_id: str, status: str, result_meta: dict = None, error_msg: str = None):
  """
  NestJS: KnowledgeController.handleWebhook í˜¸ì¶œ
  """
  url = f"{settings.NEST_API_URL}/api/v1/upload/knowledge-docs/webhook"
  
  # NestJS RagWebhookDto êµ¬ì¡°ì— ë§ì¶¤
  payload = {
      "docId": doc_id,
      "status": status, # 'COMPLETED' | 'FAILED'
  }

  if result_meta:
      payload["resultMeta"] = result_meta
  
  if error_msg:
      payload["errorMessage"] = error_msg

  headers = {
      "x-webhook-secret": settings.INTERNAL_WEBHOOK_SECRET,
      "Content-Type": "application/json"
  }

  async with httpx.AsyncClient() as client:
      try:
          resp = await client.post(url, json=payload, headers=headers, timeout=10.0)
          if resp.status_code in [200, 201]:
              logger.info(f"ğŸ”” Webhook Success: {doc_id} -> {status}")
          else:
              logger.error(f"âš ï¸ Webhook Failed: {resp.status_code} - {resp.text}")
      except Exception as e:
          logger.error(f"âŒ Webhook Connection Error: {e}")

async def process_knowledge_job(payload_json: str):
  """
  ë¡œì§ ì •ë¦¬
  1. Payload íŒŒì‹±
  2. MinIO ë‹¤ìš´ë¡œë“œ
  3. RAG ë²¡í„°í™” (TODO)
  4. ê²°ê³¼ Webhook ì „ì†¡
  """
  doc_id = None
  try:
      # 1. Payload íŒŒì‹± (NestJS: AiTaskServiceê°€ ë³´ë‚¸ ë°ì´í„°)
      task_data = json.loads(payload_json)
      doc_id = task_data.get("docId")
      minio_key = task_data.get("minioKey")
      bucket_name = task_data.get("minioBucket")
      # mime_type = task_data.get("mimeType")

      logger.info(f"ğŸ“š [Start] RAG Job | DocID: {doc_id}")

      # 2. MinIO íŒŒì¼ ë‹¤ìš´ë¡œë“œ
      logger.info(f"ğŸ“¥ Downloading: {minio_key} ({bucket_name})")
      file_content = await minio_client.get_file_content(
          object_name=minio_key, 
          bucket_name=bucket_name
      )
      
      if not file_content:
          raise ValueError("File content is empty")

      text_content = file_content.decode("utf-8")

      file_size_kb = len(file_content) / 1024
      logger.info(f"âœ… Downloaded: {file_size_kb:.2f} KB")
      logger.info("ğŸ·ï¸ Extracting Metadata via LLM...")
      extracted_meta = await extract_metadata_from_llm(text_content)
      logger.info(f"ğŸ·ï¸ Extracted: {extracted_meta}")

      # 3. ì²œí‚¹ (í—¤ë” ê¸°ì¤€, ë¬¸ììˆ˜ ê¸°ì¤€)
      # 3-1. í—¤ë” ê¸°ì¤€ í¬ê²Œ ìë¥´ê¸° 
      headers_to_split_on = [
        ("#", "Header 1"),
        ("##", "Header 2"),
        ("###", "Header 3"),
        ("####", "Header 4"),
      ]
      markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
      md_header_splits = markdown_splitter.split_text(text_content)
      
      # 3-2. ë¬¸ììˆ˜ ê¸°ì¤€ ì‘ì€ ë‹¨ìœ„ë¡œ ì¬ê·€ ìë¥´ê¸°
      text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
      )
      final_chunks = text_splitter.split_documents(md_header_splits)

      chunk_texts = [chunk.page_content for chunk in final_chunks]
      logger.info(f"ğŸ§© Chunking Complete: {len(chunk_texts)} chunks generated.")

      # 4. ì„ë² ë”© ìƒì„±
      embeddings = await get_embeddings(chunk_texts)
      logger.info(f"ğŸ§  Embedding Complete: {len(embeddings)} vectors generated.")

      # 5. DB ì €ì¥
      # async with AsyncSessionLocal() as db:
      vector_docs = []
      for idx, (chunk_obj, vector) in enumerate(zip(final_chunks, embeddings)):
        combined_meta = {
          **chunk_obj.metadata, # 
          **extracted_meta,
        }

        vector_docs.append(VectorizedDoc(
          chunk_index=idx,
          content=chunk_obj.page_content,
          meta_data=combined_meta,
          token_count=len(chunk_obj.page_content),
          embedding=vector,
          embedding_model=settings.OPENROUTER_EMBEDDING_MODEL,
          knowledge_doc_id=doc_id,
          uploader_id=task_data.get("uploaderId")
        ))
      # db.add_all(vector_docs)
      # await db.commit()
      # logger.info(f"ğŸ’¾ DB Insert Complete: {len(vector_docs)} rows.")

      async with AsyncSessionLocal() as db:
        try:
            db.add_all(vector_docs)
            await db.commit()
            logger.info(f"ğŸ’¾ DB Insert Complete: {len(vector_docs)} rows.")
        except Exception as e:
            await db.rollback() # ì—ëŸ¬ ë‚˜ë©´ ë¡¤ë°±
            raise e # ì—ëŸ¬ ë‹¤ì‹œ ë˜ì ¸ì„œ ë°”ê¹¥ try-exceptì— ì¡íˆê²Œ í•¨
      
      # 6. ì„±ê³µ Webhook
      await send_webhook(
          doc_id=doc_id, 
          status="COMPLETED", 
          result_meta={
            "chunkCount": len(final_chunks),
            "embeddingModel": settings.OPENROUTER_EMBEDDING_MODEL,
          }
      )
      logger.info(f"âœ… Job Finished: {doc_id}")

  except Exception as e:
      logger.error(f"âŒ Job Failed ({doc_id}): {e}")
      # ì‹¤íŒ¨ Webhook (doc_idê°€ ìˆì„ ë•Œë§Œ)
      if doc_id:
          await send_webhook(
              doc_id=doc_id, 
              status="FAILED", 
              error_msg=str(e)
          )

async def run_knowledge_worker():
  """
  Redis Queue ë¦¬ìŠ¤ë„ˆ (Queue Name: ai:job:queue)
  """
  logger.info("ğŸš€ Knowledge Worker Listening on 'ai:job:queue'...")
  redis_client = get_redis_client()
  
  try:
      while True:
          await semaphore.acquire()
          
          # NestJSê°€ ë„£ëŠ” í ì´ë¦„ê³¼ ì¼ì¹˜í•´ì•¼ í•¨
          result = await redis_client.brpop("ai:job:queue", timeout=5)

          if result:
              logger.info(f"âœ… [BRPOP] Job received: {result}")
              _, payload = result
              # bytes to string
              if isinstance(payload, bytes):
                  payload = payload.decode('utf-8')
              
              # ë¹„ë™ê¸° Task ì‹¤í–‰
              task = asyncio.create_task(process_knowledge_job(payload))
              task.add_done_callback(lambda t: semaphore.release())
          else:
              semaphore.release()
              await asyncio.sleep(0.1)

  except asyncio.CancelledError:
      logger.info("ğŸ›‘ Worker Cancelled")
  finally:
      await redis_client.close()